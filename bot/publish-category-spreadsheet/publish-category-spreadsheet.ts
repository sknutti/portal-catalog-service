import { CatalogResolver } from '@bot/publish-category-spreadsheet/catalog-resolver';
import { keyWith, UnexpectedError } from '@dsco/ts-models';
import { CoreCatalog } from '@lib/core-catalog';
import { getDscoEnv, getFanaticsBucketName, getIsRunningLocally } from '@lib/environment';
import { getFanaticsAccountForEnv } from '@lib/fanatics';
import {
    CatalogSpreadsheetS3Metadata,
    downloadS3Bucket,
    downloadS3Metadata,
    parseCatalogItemS3UploadUrl,
    writeS3Object,
} from '@lib/s3';
import { DscoSpreadsheet, generateDscoSpreadsheet, PhysicalSpreadsheet, XlsxSpreadsheet } from '@lib/spreadsheet';
import { CsvSpreadsheet } from '@lib/spreadsheet/physical-spreadsheet/csv-spreadsheet';
import { gzipAsync, loadCatalogItemsFromMongo, randomFloat, WarehousesLoader } from '@lib/utils';
import { batch, collect, enumerate, filter, map } from '@lib/utils/iter-tools';
import { sendWebsocketEvent } from '@lib/utils/send-websocket-event';
import type { S3CreateEvent } from 'aws-lambda';
import { Err, Ok, Result } from 'ts-results';
import { CatalogSpreadsheetWebsocketEvents } from '../../api';

export interface PublishCategorySpreadsheetEvent {
    supplierId: number;
    retailerId: number;
    userId: number;
    categoryPath: string;
    s3Path: string;
    skippedRowIndexes?: number[];
    // Signifies this file was uploaded via a local test and should be skipped from automated processing
    isLocalTest?: boolean;
}

// Purposely 10 seconds before actual timeout
const LAMBDA_TIMEOUT = 890 * 1_000;

export async function publishCategorySpreadsheet(s3Event: S3CreateEvent): Promise<void> {
    const event = await getEventFromS3(s3Event);
    console.log('Publish event extracted from s3 metadata: ', event);

    // Ignore any s3 events generated by unit tests
    if (event.isLocalTest && !getIsRunningLocally()) {
        return;
    }

    try {
        const resp = await timeoutPromise(publishSpreadsheetImpl(event), LAMBDA_TIMEOUT);

        if (resp === 'timeout') {
            throw new Error('Timeout occurred publishing spreadsheet.');
        } else if (resp.ok) {
            await sendWebsocketEvent('success', resp.val, event.supplierId);
        } else {
            await publishResultIfFanatics(
                {
                    success: false,
                    error: resp.val,
                    message: resp.val.message,
                },
                event.supplierId,
            );

            await sendWebsocketEvent(
                'error',
                {
                    error: resp.val,
                    message: resp.val.message,
                    categoryPath: event.categoryPath,
                },
                event.supplierId,
            );
        }
    } catch (error: any) {
        await publishResultIfFanatics(
            {
                success: false,
                error,
                message: 'message' in error ? error.message : 'Unexpected error',
            },
            event.supplierId,
        );
        await sendWebsocketEvent(
            'error',
            {
                error,
                message: 'message' in error ? error.message : 'Unexpected error',
                categoryPath: event.categoryPath,
            },
            event.supplierId,
        );
        throw error;
    }
}

async function getEventFromS3(createEvent: S3CreateEvent): Promise<PublishCategorySpreadsheetEvent> {
    console.log('Handling s3 object created: ', createEvent.Records[0]);

    let s3Path = createEvent.Records[0].s3.object.key;
    s3Path = s3Path.replace(/\+/g, ' ');
    s3Path = decodeURIComponent(s3Path);

    const meta = await downloadS3Metadata<CatalogSpreadsheetS3Metadata>(s3Path);
    const skippedRowIndexes = meta.skipped_row_indexes
        ?.split(',')
        .map(parseInt)
        .filter((idx) => !isNaN(idx));

    const parsed = parseCatalogItemS3UploadUrl(s3Path);
    if (parsed === 'error') {
        throw new Error(`Failed parsing catalog s3 metadata. Url: ${s3Path}`);
    }
    const { supplierId, retailerId, userId } = parsed;

    if (!meta.category_path) {
        throw new Error(`S3 file ${s3Path} must have Metadata.category_path defined`);
    }

    return {
        s3Path,
        skippedRowIndexes,
        supplierId,
        retailerId,
        userId,
        categoryPath: meta.category_path,
        isLocalTest: meta.is_local_test === 'true',
    };
}

async function publishSpreadsheetImpl({
    categoryPath,
    retailerId,
    supplierId,
    userId,
    s3Path,
    skippedRowIndexes,
}: PublishCategorySpreadsheetEvent): Promise<Result<CatalogSpreadsheetWebsocketEvents['success'], UnexpectedError>> {
    const callId = Math.random().toString(36).substring(6).toUpperCase();
    console.log(`${callId} - Starting processing for supplier: ${supplierId}, path: ${s3Path}`);

    const sendProgress = (progress: number, message: string) => {
        return sendWebsocketEvent('progressUpdate', { progress, message, categoryPath }, supplierId);
    };

    const [, dscoSpreadsheet, warehouses, [supplierSpreadsheet, existingCatalogItems]] = await Promise.all([
        sendProgress(0.34, 'Parsing Spreadsheet...'),
        generateDscoSpreadsheet(supplierId, retailerId, categoryPath),
        WarehousesLoader.loadWarehouses(supplierId),
        loadSpreadsheetAndCatalogItems(categoryPath, userId, supplierId, retailerId, s3Path),
    ] as const);

    if (!(dscoSpreadsheet instanceof DscoSpreadsheet)) {
        return Err(dscoSpreadsheet);
    }

    if (!supplierSpreadsheet) {
        return Ok({
            totalRowCount: 0,
            categoryPath,
        });
    }

    // Pull the row data from the google spreadsheet
    const catalogRows = supplierSpreadsheet.extractCatalogRows(
        dscoSpreadsheet,
        supplierId,
        retailerId,
        categoryPath,
        keyWith(existingCatalogItems, (item) => [item.sku!, item]),
        warehouses,
    );

    // Resolve the rows that were modified, giving progress updates
    const resolver = new CatalogResolver(supplierId, userId);

    const skippedRows = new Set(skippedRowIndexes);

    const totalRowCount = supplierSpreadsheet.numDataRows();
    let remainingRowsToValidate = totalRowCount;

    // Enumerate all of the rows starting at 1 for the header.  Then filter out the skipped rows, rows without data, and unmodified rows
    const rowsToSave = filter(enumerate(catalogRows, 1), ([row, rowIdx]) => {
        const needsSave = !row.emptyRow && row.modified && !skippedRows.has(rowIdx);

        if (!needsSave) {
            remainingRowsToValidate -= 1;
        }

        return needsSave;
    });

    let batchSize = 20;
    if (remainingRowsToValidate > 5_000) {
        batchSize = 100;
    }

    const numConcurrentGearmanCalls = 15;
    const startValidationPct = randomFloat(0.45, 0.55);

    // Send batches of {batchSize} items to gearman at once
    const resolvedBatches = map(batch(rowsToSave, batchSize), (rows) => resolver.resolveBatch(rows));

    // Batch the gearman calls to run as many as we can concurrently
    const gearmanCalls = map(batch(resolvedBatches, numConcurrentGearmanCalls), (gearmanCalls) =>
        Promise.all(collect(gearmanCalls)),
    );

    await sendProgress(startValidationPct, `Validating ${remainingRowsToValidate} modified rows...`);

    for await (const resolvedBatch of gearmanCalls) {
        for (const resolvedBatchError of resolvedBatch) {
            if (resolvedBatchError) {
                await publishResultIfFanatics(
                    {
                        success: false,
                        rowWithError: resolvedBatchError.rowIdx,
                        validationMessages: resolvedBatchError.messages,
                    },
                    supplierId,
                );

                return Ok({
                    totalRowCount,
                    validationMessages: resolvedBatchError.messages,
                    rowWithError: resolvedBatchError.rowIdx,
                    sentRequest: await gzipAsync(Buffer.from(JSON.stringify(resolvedBatchError.sentRequest), 'utf8')),
                    categoryPath,
                });
            } else {
                remainingRowsToValidate -= batchSize;

                // Can happen if skipped rows.
                if (remainingRowsToValidate < 0) {
                    remainingRowsToValidate = 0;
                }
            }
        }

        const validationPct = (totalRowCount - remainingRowsToValidate) / totalRowCount;

        await sendProgress(
            (1 - startValidationPct) * validationPct + startValidationPct,
            `Validating ${numberWithCommas(remainingRowsToValidate)} rows...`,
        );

        console.log(
            `${callId} - Finished processing ${numberWithCommas(
                totalRowCount - remainingRowsToValidate,
            )} out of ${numberWithCommas(totalRowCount)} rows...`,
        );
    }

    console.log(`${callId} - Finished!`);

    await publishResultIfFanatics(
        {
            success: true,
            numUploadedItems: totalRowCount,
        },
        supplierId,
    );

    return Ok({ totalRowCount, categoryPath });
}

/**
 * For every sku in the spreadsheet, we try loading the existing catalog items.
 * This allows us to merge uploaded data with existing catalog data, and detect which rows have changed
 */
async function loadSpreadsheetAndCatalogItems(
    categoryPath: string,
    userId: number,
    supplierId: number,
    retailerId: number,
    s3Path: string,
): Promise<[PhysicalSpreadsheet | undefined, CoreCatalog[]]> {
    const buffer = await downloadS3Bucket(s3Path);

    const supplierSpreadsheet = XlsxSpreadsheet.isXlsx(buffer)
        ? XlsxSpreadsheet.fromBuffer(buffer)
        : new CsvSpreadsheet(buffer);

    return [supplierSpreadsheet, await loadCatalogItemsFromMongo(supplierId, 'sku', supplierSpreadsheet?.skus() ?? [])];
}

/**
 * Fanatics is using an s3 bucket to communicate with us - this writes the results back to that bucket so they can pick it up
 */
async function publishResultIfFanatics(message: any, supplierId: number) {
    if (getIsRunningLocally() || supplierId !== getFanaticsAccountForEnv()?.supplierId) {
        return;
    }

    await writeS3Object(getFanaticsBucketName(), `${getDscoEnv()}-result.json`, JSON.stringify(message));
}

// Resolves just before the lambda would time out.  This allows us to send it back to the user over the socket
function timeoutPromise<T>(promise: Promise<T>, timeout: number): Promise<'timeout' | T> {
    return new Promise((resolve, reject) => {
        // Set up the timeout
        const timer = setTimeout(() => {
            resolve('timeout');
        }, timeout);

        // Set up the real work
        promise
            .then((value) => {
                clearTimeout(timer);
                resolve(value);
            })
            .catch((error) => {
                clearTimeout(timer);
                reject(error);
            });
    });
}

function numberWithCommas(x: number): string {
    return x.toString().replace(/\B(?=(\d{3})+(?!\d))/g, ',');
}
